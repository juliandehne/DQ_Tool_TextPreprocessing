---
title: "Comparing Tools and Workflows for Data Quality in Text Preprocessing"
author: "Yannik Peters"
format: html
editor: visual
bibliography: references.bib
---

# 1. Introduction

The digitalisation has led to an innovation of research objects and research methods. While statistical methods to analyze numerical data have a long tradition, it is especially the automated analysis of text data that has seen huge improvement in recent years. Automated text analysis methods are applied to various data sources, be it social media data, news paper articles, parliamentary speeches, historical texts or literature. In this introduction, we want to focus on an important, necessary and often times challenging aspect related to data quality in text data: the text processing. Text processing can be defined as all changes that are done to the text data after the data collection and before the data analysis. Its main purpose is to bring the raw data in a form that is then suitable for applying specific research methods, but also to reduce the probability of errors. In this sense, text processing is heavily related to the measurement dimension of data quality. On the one hand, text processing can help to reduce measurement errors, by increasing consistency or accuracy. On the other hand, text processing itself can become a source for potential errors. In the TED-On, the "Total Error Framework for Digital Traces of Human Behavior On Online Platforms" [@sen2021] these errors are referred to as `trace reduction errors`. According to Sen et al. an example for this error would be: "Researchers might decide to use a classifier that removes spam, but has a high false positive rate, inadvertently removing non-spam tweets. They might likewise discard tweets without textual content, thereby ignoring relevant statements made through hyper-links or embedded pictures/videos" (p. 413).

In this tutorial we want to go through all the classic steps of text processing and compare as well as recommend different packages and tools to use. The final step is gonna be to create a document-feature-matrix (DFM) or more precise a document-term-matrix (DTM), that is used often times for multiple analysis techniques such as sentiment analysis or topic modeling. For this purpose, we created a small social media data set with posts about the Olympic summer games in Paris 2024. The Olympic summer games can be considered a transnational media event [@hepp2015], which is nowadays of course not only covered by traditional media but is communicatively accompanied on social media. For copyright reasons, we have constructed an artificial data set which does not contain any real content.

# 2. Set up

At first, we will open all relevant libraries. Please make sure to have all relevant packages installed using the `install.packages()` command. We will be using and comparing some of the most important text preprocessing and analysis R packages like `quanteda`, `stringr`, `textclean` or `tm`. We will also use specific packages like `skimr`, `spelling`, `polyglotr` or `deeplr` for very specific purposes.

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(quanteda)
library(textclean)
library(tm)
library(textTinyR)
library(skimr)
library(readr)
library(dplyr)
library(spelling) 
library(hunspell)
library(polyglotr)
library(deeplr)
```

Finally, we will then load our artificial dataset from the Olympic summer games.

```{r message = FALSE, warning = FALSE}
olympics_data <- read_csv("6.modified_olympics_tweets_RT.csv", locale = locale(encoding = "Latin1"))
olympics_data
```

# 3. Application of tools and use case

### 3.1 Basic data (quality) checks

Let's start with checking out the basic structure of our dataset.

```{r message = FALSE, warning = FALSE}
str(olympics_data)
```

Here we find 13 variables and 150 observations.

```{r message = FALSE, warning=FALSE}
summary(olympics_data)
```

When running the `summary()` function, the overview might get a bit messy depending on the size of the dataframe. We therefore recommend R tools, that offer basic data quality reports and arrange the results clearly like the `skimr` package. Here, you can receive an overview of the various variables of our data set including descriptive statistics and missing values.

```{r warning=FALSE}
skim(olympics_data)
```

### 3.2 Dealing with multiple languages

As the Olympic games are a transnational media event, it does not come as a surprise to receive a multilingual data set. The basic problem about multilingual data sets is that common CSS research methods like topic modeling or sentiment analysis are expected to be in one language. There are different strategies to deal with multilingual data sets. The chosen strategy has to depend on the specific contexts, the applied methods and the research design and questions. We can basically distinguish three or four main strategies with regard to multilingual data sets (see @haukelicht2023, @lind2021a). For this short version, we will discuss three main strategies.

1\) Selecting cases: one language

This approach is basically about choosing cases that only contain documents in one language. For our Twitter/X data set, we could for instance remove all postings which are not English ones.

```{r warning=FALSE, message=FALSE}
olympics_data_en <- olympics_data %>% filter(language == "en")
table(olympics_data_en$language)
```

Of course, this strategy might result in a representation error as we systematically exclude specific content from analysis (in our case twenty tweets). So let's take a look at the other strategies.

2\) Multiple single language data sets

Another way of dealing with multilingual data sets is to create language specific subsamples of our data. The main advantage of this strategy is, that we do not lose any content due to exclusion or translation errors. However, compared to the other methods there are more validation steps required with regard to the each single language data set (for detailed information see @haukelicht2023, @lind2021a). As we have already created a data set which only contains English tweets, we will create two more dataframes for German and French tweets.

```{r warning=FALSE, message=FALSE}
olympics_data_de <- olympics_data %>% filter(language == "de")
table(olympics_data_de$language)

olympics_data_fr <- olympics_data %>% filter(language == "fr")
table(olympics_data_fr$language)
```

We only find a few documents that are not in English resulting in some very small language sets. Therefore, this strategy might not be the best with regard to our example data.

3\) Translating

The third option of dealing with multilingual datasets is to translate the non-English speaking tweets into English. As this is a just a small, artificial, sample data set, we could actually translate the few tweets manually. In a real case scenario however, analyzing a data set of millions of tweets, you would usually use a automated translation algorithm or method. The most common translation tools are `Google Translator` and `DeepL`. The main advantage of the translation method is to generate one singular data set, which can then be analyzed with one model only. This requires less resources all well. The main disadvantage lies in the potential of translation errors. It is therefore necessary to evaluate your translation method. For this purpose, let's translate all non-English comments with both tools in order to compare the results. First, we will use the `polyglotr` and the `deeplr` package to translate the German text.

```{r message=FALSE, warning=FALSE}
#Translation of German posts and creation of translated dataframe using Google Translate
translation_google_de <- google_translate(olympics_data_de$tweet_text, target_language = "en", source_language = "de")
translation_google_de <- sapply(translation_google_de, function(x) x[[1]])
olympics_data_de_google <- olympics_data_de
olympics_data_de_google$tweet_text <- translation_google_de
```

To access the DeepL API, you currently need a developer account. You can use this [link](https://www.deepl.com/en/pro/change-plan#developer) to get to the registration page. A free account can translate up to 500,000 characters per month and gives you access to the DeepL Rest API. In order to translate our text using DeepL in R, you first need this API-key. When signing up for a developer account, you automatically receive such a key.

```{r message=FALSE, warning=FALSE, echo=FALSE}
my_key <- "e1b84858-e71d-4be5-a592-f67a6ffb35ea:fx"
```

```{r warning=FALSE, message=FALSE, results='hide'}
#Translation of German posts and creation of translated dataframe using DeepL
translation_deepl_de <- translate2(olympics_data_de$tweet_text, target_lang = "EN", auth_key = my_key)
olympics_data_de_deepl <- olympics_data_de
olympics_data_de_deepl$tweet_text <- translation_deepl_de
```

::: callout-note
For this code to work, make sure that you create a my_key object containing your API key.

my_key \<- "Your key"
:::

Let's compare the results for the German tweets.

```{r}
head(olympics_data_de_google$tweet_text)
head(olympics_data_de_deepl$tweet_text)
```

We can see from a quick comparison that the translations seem pretty similar. We can also use certain metric to determine the degree of similarity. For this example, we will apply [cosine similarity](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50). First, we have to `unlist` our text data, as the `COS_TEXT` function of the `textTidyR` package requires a vector as an input.

```{r}
google_translation_de <- unlist(translation_google_de)
deepl_translation_de <- unlist(translation_deepl_de)


cosine_similarities <- COS_TEXT(google_translation_de, deepl_translation_de, separator = " ")

cosine_similarities
mean(cosine_similarities)
```

With a mean cosine similarity of 0.82 on a scale from 0 to 1, the translations of Google Translate and DeepL are indeed very similar. We can do the very same for the French postings.

```{r message=FALSE, warning=FALSE}
#Translation of French posts and creation of translated dataframe using Google Translate
translation_google_fr <- google_translate(olympics_data_fr$tweet_text, target_language = "en", source_language = "fr")
translation_google_fr <- sapply(translation_google_fr, function(x) x[[1]])
olympics_data_fr_google <- olympics_data_fr
olympics_data_fr_google$tweet_text <- translation_google_fr

#Translation of French posts and creation of translated dataframe using DeepL
translation_deepl_fr <- translate2(olympics_data_fr$tweet_text, target_lang = "EN", auth_key = my_key)
olympics_data_fr_deepl <- olympics_data_fr
olympics_data_fr_deepl$tweet_text <- translation_deepl_fr

#compare Google Translate und Deepl translation manually
olympics_data_fr_google$tweet_text
olympics_data_fr_deepl$tweet_text

#unlist french translation data
google_translation_fr <- unlist(translation_google_fr)
deepl_translation_fr <- unlist(translation_deepl_fr)

#calculate cosine similarities for French translation data
cosine_similarities <- COS_TEXT(google_translation_fr, deepl_translation_fr, separator = " ")

cosine_similarities
mean(cosine_similarities)
```

With an average cosine similarity of 0.86, it is slightly higher for the French than the German translation.

The key question for us is now: Do we want to use the translation of DeepL or Google Translate? Generally, DeepL is considered to be more accurate then Google Translate (like [here](https://www.geeksforgeeks.org/deepl-vs-google/)). In current research, both tools are considered applicable. For the translation of Spanish idiomatic expressions into English, @hidalgo-ternero2021 find DeepL with an average accuracy rate of 89% slighty better than Google Translate with 86%. @sebo2024 do not find significant differences in the accuracy of both tools. One of the major advantages of Google Translate is that it can be applied to significantly more languages than DeepL. As we only have two languages to translate in our case, we will use the DeepL translation here.

```{r}
olympics_data_en_full <- rbind(olympics_data_en, olympics_data_de_deepl, olympics_data_fr_deepl)
```

### 3.3 Minor text operations: removing special characters

### 3.4 Removing stopwords

In text preprocessing of social media data, there are often times certain typical preprocessing steps included like removing special characters or changing capital letters to lower cases.

### 3.5 Identifying misspelling

Now, we have created one data set with only English language postings. Next, we want to check is whether the text is correct in terms of spelling. Spelling errors are problematic with regard to text data quality as specific words might not be considered as equal to the correct word by specific methods. We therefore want to check the text data for errors. In R, there are many packages for spelling corrections.

```{r warning=FALSE, message=FALSE}
 
# Function to check spelling and return misspelled words with line numbers and corrections on the basis of hunspell dictionary

check_spelling_and_correct <- function(text) { 

# Use spell_check_text function from the spelling package 
misspelled <- spell_check_text(text) 

if (nrow(misspelled) > 0) { 

# Create a string of misspelled words with their positions 
misspelled_str <- paste(sapply(1:nrow(misspelled), function(i) { 

paste0(misspelled$word[i], " (", paste(misspelled$found[[i]], collapse = ","), ")") 
}), collapse = "; ") 


# Generate suggestions for misspelled words 

suggestions <- hunspell_suggest(misspelled$word) 

corrections <- sapply(suggestions, function(x) ifelse(length(x) > 0, x[1], NA)) 


# Create a string of corrections 

corrections_str <- paste(corrections, collapse = "; ") 

return(list(misspelled = misspelled_str, corrections = corrections_str)) 

} else { 

return(list(misspelled = NA_character_, corrections = NA_character_))}} 

# Apply the spelling check and correction to each tweet 

olympics_data_new <- olympics_data_en_full %>% 

mutate(Spelling_Check = lapply(tweet_text, check_spelling_and_correct), 

Misspelled_Words = sapply(Spelling_Check, function(x) x$misspelled), 

Corrected_Spellings = sapply(Spelling_Check, function(x) x$corrections)) %>% 

select(-Spelling_Check)  # Remove the intermediate column 

# View the results 

head(olympics_data_new[c("tweet_text", "Misspelled_Words", "Corrected_Spellings")]) 

```

### 3.6 Creating a DFM: tokenization and lemmatization

### 3.7 LDA Analysis

# 4. Discussion

# 5. Literature
